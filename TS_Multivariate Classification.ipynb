{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('datatraining.txt', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
    "df2= pd.read_csv('datatest.txt', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
    "df3= pd.read_csv('datatest2.txt', header=0, parse_dates=[0], index_col=0, squeeze=True)\n",
    "\n",
    "df4= pd.merge(df2, df1, how='outer')\n",
    "df_m=pd.merge(df4, df3, how='outer')\n",
    "\n",
    "df = df_m.set_index(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, class_name):\n",
    "    df = remove_missing_values(df)\n",
    "    numeric_columns = get_numeric_columns(df)\n",
    "    rdf = df.copy(deep=True)\n",
    "    df, feature_names, class_values = one_hot_encoding(df, class_name)\n",
    "    real_feature_names = get_real_feature_names(rdf, numeric_columns, class_name)\n",
    "    rdf = rdf[real_feature_names + (class_values if isinstance(class_name, list) else [class_name])]\n",
    "    features_map = get_features_map(feature_names, real_feature_names)\n",
    "\n",
    "    return df, feature_names, class_values, numeric_columns, rdf, real_feature_names, features_map\n",
    "\n",
    "def remove_missing_values(df):\n",
    "    for column_name, nbr_missing in df.isna().sum().to_dict().items():\n",
    "        if nbr_missing > 0:\n",
    "            if column_name in df._get_numeric_data().columns:\n",
    "                mean = df[column_name].mean()\n",
    "                df[column_name].fillna(mean, inplace=True)\n",
    "            else:\n",
    "                mode = df[column_name].mode().values[0]\n",
    "                df[column_name].fillna(mode, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_numeric_columns(df):\n",
    "    numeric_columns = list(df._get_numeric_data().columns)\n",
    "    return numeric_columns\n",
    "\n",
    "def get_real_feature_names(rdf, numeric_columns, class_name):\n",
    "    real_feature_names = [c for c in rdf.columns if c in numeric_columns and c != class_name]\n",
    "    real_feature_names += [c for c in rdf.columns if c not in numeric_columns and c != class_name]\n",
    "    return real_feature_names\n",
    "\n",
    "def one_hot_encoding(df, class_name):\n",
    "    dfX = pd.get_dummies(df[[c for c in df.columns if c != class_name]], prefix_sep='=')\n",
    "    class_name_map = {v: k for k, v in enumerate(sorted(df[class_name].unique()))}\n",
    "    dfY = df[class_name].map(class_name_map)\n",
    "    df = pd.concat([dfX, dfY], axis=1)\n",
    "    feature_names = list(dfX.columns)\n",
    "    class_values = sorted(class_name_map)\n",
    "    return df, feature_names, class_values\n",
    "\n",
    "def get_features_map(feature_names, real_feature_names):\n",
    "    features_map = defaultdict(dict)\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    while i < len(feature_names) and j < len(real_feature_names):\n",
    "        if feature_names[i] == real_feature_names[j]:\n",
    "            features_map[j][feature_names[i]] = j\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif feature_names[i].startswith(real_feature_names[j]):\n",
    "            features_map[j][feature_names[i]] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return features_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Light</th>\n",
       "      <th>CO2</th>\n",
       "      <th>HumidityRatio</th>\n",
       "      <th>Occupancy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-02-02 14:19:00</th>\n",
       "      <td>23.7000</td>\n",
       "      <td>26.272</td>\n",
       "      <td>585.200000</td>\n",
       "      <td>749.200000</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-02 14:19:59</th>\n",
       "      <td>23.7180</td>\n",
       "      <td>26.290</td>\n",
       "      <td>578.400000</td>\n",
       "      <td>760.400000</td>\n",
       "      <td>0.004773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-02 14:21:00</th>\n",
       "      <td>23.7300</td>\n",
       "      <td>26.230</td>\n",
       "      <td>572.666667</td>\n",
       "      <td>769.666667</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-02 14:22:00</th>\n",
       "      <td>23.7225</td>\n",
       "      <td>26.125</td>\n",
       "      <td>493.750000</td>\n",
       "      <td>774.750000</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-02 14:23:00</th>\n",
       "      <td>23.7540</td>\n",
       "      <td>26.200</td>\n",
       "      <td>488.600000</td>\n",
       "      <td>779.000000</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Temperature  Humidity       Light         CO2  \\\n",
       "date                                                                 \n",
       "2015-02-02 14:19:00      23.7000    26.272  585.200000  749.200000   \n",
       "2015-02-02 14:19:59      23.7180    26.290  578.400000  760.400000   \n",
       "2015-02-02 14:21:00      23.7300    26.230  572.666667  769.666667   \n",
       "2015-02-02 14:22:00      23.7225    26.125  493.750000  774.750000   \n",
       "2015-02-02 14:23:00      23.7540    26.200  488.600000  779.000000   \n",
       "\n",
       "                     HumidityRatio  Occupancy  \n",
       "date                                           \n",
       "2015-02-02 14:19:00       0.004764          1  \n",
       "2015-02-02 14:19:59       0.004773          1  \n",
       "2015-02-02 14:21:00       0.004765          1  \n",
       "2015-02-02 14:22:00       0.004744          1  \n",
       "2015-02-02 14:23:00       0.004767          1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = 'Occupancy'\n",
    "res = prepare_dataset(df, class_name)\n",
    "df, feature_names, class_values, numeric_columns, rdf, real_feature_names, features_map = res\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "from tslearn.shapelets import ShapeletModel\n",
    "from tslearn.shapelets import grabocka_params_to_shapelet_size_dict\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(257, 80, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#costruisco il set of multivariate time series\n",
    "\n",
    "samples = np.zeros((257,80,6))\n",
    "length = 80\n",
    "# step over the 5,000 in jumps of 200\n",
    "j = 0\n",
    "for i in range(0,20560,length):\n",
    "    samples[j] = df[i:i+length]\n",
    "    j += 1\n",
    "print(len(samples),type(samples))\n",
    "\n",
    "X = np.array(samples)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.37000000e+01, 2.62720000e+01, 5.85200000e+02, 7.49200000e+02,\n",
       "         4.76416302e-03, 1.00000000e+00],\n",
       "        [2.37180000e+01, 2.62900000e+01, 5.78400000e+02, 7.60400000e+02,\n",
       "         4.77266099e-03, 1.00000000e+00],\n",
       "        [2.37300000e+01, 2.62300000e+01, 5.72666667e+02, 7.69666667e+02,\n",
       "         4.76515255e-03, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.32900000e+01, 2.85000000e+01, 4.64000000e+02, 1.10800000e+03,\n",
       "         5.04422920e-03, 1.00000000e+00],\n",
       "        [2.32150000e+01, 2.86333333e+01, 4.62333333e+02, 1.12783333e+03,\n",
       "         5.04492265e-03, 1.00000000e+00],\n",
       "        [2.32000000e+01, 2.86000000e+01, 4.69000000e+02, 1.12875000e+03,\n",
       "         5.03439915e-03, 1.00000000e+00]],\n",
       "\n",
       "       [[2.32000000e+01, 2.86500000e+01, 4.69000000e+02, 1.12425000e+03,\n",
       "         5.04327191e-03, 1.00000000e+00],\n",
       "        [2.32000000e+01, 2.86200000e+01, 4.69000000e+02, 1.13040000e+03,\n",
       "         5.03794823e-03, 1.00000000e+00],\n",
       "        [2.32000000e+01, 2.87000000e+01, 4.64000000e+02, 1.13800000e+03,\n",
       "         5.05214492e-03, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.26000000e+01, 2.52000000e+01, 4.39800000e+02, 8.60600000e+02,\n",
       "         4.27236993e-03, 1.00000000e+00],\n",
       "        [2.26000000e+01, 2.52000000e+01, 4.35500000e+02, 8.64000000e+02,\n",
       "         4.27236993e-03, 1.00000000e+00],\n",
       "        [2.26000000e+01, 2.51500000e+01, 4.30750000e+02, 8.59000000e+02,\n",
       "         4.26383489e-03, 1.00000000e+00]],\n",
       "\n",
       "       [[2.26000000e+01, 2.51400000e+01, 4.40600000e+02, 8.54400000e+02,\n",
       "         4.26212792e-03, 1.00000000e+00],\n",
       "        [2.26000000e+01, 2.51500000e+01, 4.28200000e+02, 8.47400000e+02,\n",
       "         4.26383489e-03, 1.00000000e+00],\n",
       "        [2.26000000e+01, 2.50400000e+01, 4.32200000e+02, 8.37200000e+02,\n",
       "         4.24505864e-03, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.22000000e+01, 2.47900000e+01, 0.00000000e+00, 7.61500000e+02,\n",
       "         4.10073727e-03, 0.00000000e+00],\n",
       "        [2.22000000e+01, 2.47900000e+01, 0.00000000e+00, 7.61000000e+02,\n",
       "         4.10073727e-03, 0.00000000e+00],\n",
       "        [2.22000000e+01, 2.47360000e+01, 0.00000000e+00, 7.57000000e+02,\n",
       "         4.09174588e-03, 0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.07450000e+01, 2.62900000e+01, 0.00000000e+00, 1.48700000e+03,\n",
       "         3.97730963e-03, 0.00000000e+00],\n",
       "        [2.07300000e+01, 2.62600000e+01, 0.00000000e+00, 1.49400000e+03,\n",
       "         3.96904997e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.62450000e+01, 0.00000000e+00, 1.50650000e+03,\n",
       "         3.98154635e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.07900000e+01, 2.66500000e+01, 0.00000000e+00, 1.47550000e+03,\n",
       "         4.04338703e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.66500000e+01, 0.00000000e+00, 1.47700000e+03,\n",
       "         4.04338703e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.66000000e+01, 0.00000000e+00, 1.47700000e+03,\n",
       "         4.03575172e-03, 0.00000000e+00]],\n",
       "\n",
       "       [[2.07900000e+01, 2.67000000e+01, 0.00000000e+00, 1.47300000e+03,\n",
       "         4.05102253e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.67000000e+01, 0.00000000e+00, 1.47300000e+03,\n",
       "         4.05102253e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.67000000e+01, 0.00000000e+00, 1.47300000e+03,\n",
       "         4.05102253e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.07000000e+01, 2.72000000e+01, 0.00000000e+00, 1.42150000e+03,\n",
       "         4.10442230e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.72000000e+01, 0.00000000e+00, 1.40400000e+03,\n",
       "         4.10442230e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.72000000e+01, 0.00000000e+00, 1.40450000e+03,\n",
       "         4.10442230e-03, 0.00000000e+00]],\n",
       "\n",
       "       [[2.07000000e+01, 2.71666667e+01, 0.00000000e+00, 1.40866667e+03,\n",
       "         4.09935922e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.72000000e+01, 0.00000000e+00, 1.40000000e+03,\n",
       "         4.10442230e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.72000000e+01, 0.00000000e+00, 1.40100000e+03,\n",
       "         4.10442230e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.08900000e+01, 2.77450000e+01, 4.23500000e+02, 1.52150000e+03,\n",
       "         4.23681810e-03, 1.00000000e+00],\n",
       "        [2.08900000e+01, 2.80225000e+01, 4.18750000e+02, 1.63200000e+03,\n",
       "         4.27948547e-03, 1.00000000e+00],\n",
       "        [2.10000000e+01, 2.81000000e+01, 4.09000000e+02, 1.86400000e+03,\n",
       "         4.32073200e-03, 1.00000000e+00]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faccio il clustering sulla X costruita nella cella precedente\n",
    "from tslearn.piecewise import OneD_SymbolicAggregateApproximation\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "n_sax_symbols_avg = 10\n",
    "n_sax_symbols_slope = 5\n",
    "d1_sax = OneD_SymbolicAggregateApproximation(\n",
    "    n_segments=10,\n",
    "    alphabet_size_avg=n_sax_symbols_avg,\n",
    "    alphabet_size_slope=n_sax_symbols_slope)\n",
    "X_d1 = d1_sax.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesKMeans(dtw_inertia=False, init='k-means++', max_iter=5,\n",
       "                 max_iter_barycenter=100, metric='euclidean',\n",
       "                 metric_params=None, n_clusters=3, n_init=1, n_jobs=None,\n",
       "                 random_state=0, tol=1e-06, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_d1 = TimeSeriesKMeans(n_clusters=3, metric=\"euclidean\", max_iter=5, random_state=0)\n",
    "km_d1.fit(X_d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 2, 2,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 0,\n",
       "       0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = km_d1.labels_\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 80, 6) (78, 80, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2.09633333e+01, 2.50333333e+01, 0.00000000e+00, 5.34000000e+02,\n",
       "         3.83756341e-03, 0.00000000e+00],\n",
       "        [2.10000000e+01, 2.50000000e+01, 0.00000000e+00, 5.32000000e+02,\n",
       "         3.84112401e-03, 0.00000000e+00],\n",
       "        [2.10000000e+01, 2.50000000e+01, 0.00000000e+00, 5.34500000e+02,\n",
       "         3.84112401e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.07900000e+01, 2.57633333e+01, 0.00000000e+00, 5.21666667e+02,\n",
       "         3.90801510e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.57450000e+01, 0.00000000e+00, 5.26500000e+02,\n",
       "         3.90521668e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.57950000e+01, 0.00000000e+00, 5.19500000e+02,\n",
       "         3.91284881e-03, 0.00000000e+00]],\n",
       "\n",
       "       [[2.10000000e+01, 2.52450000e+01, 0.00000000e+00, 5.74000000e+02,\n",
       "         3.87900178e-03, 0.00000000e+00],\n",
       "        [2.10000000e+01, 2.47666667e+01, 0.00000000e+00, 5.71666667e+02,\n",
       "         3.80505420e-03, 0.00000000e+00],\n",
       "        [2.10000000e+01, 2.46000000e+01, 0.00000000e+00, 5.71000000e+02,\n",
       "         3.77929260e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.10000000e+01, 2.42666667e+01, 0.00000000e+00, 5.78666667e+02,\n",
       "         3.72777576e-03, 0.00000000e+00],\n",
       "        [2.10000000e+01, 2.42900000e+01, 0.00000000e+00, 5.79000000e+02,\n",
       "         3.73138166e-03, 0.00000000e+00],\n",
       "        [2.08900000e+01, 2.51000000e+01, 0.00000000e+00, 5.79000000e+02,\n",
       "         3.83042432e-03, 0.00000000e+00]],\n",
       "\n",
       "       [[2.29083333e+01, 3.06000000e+01, 4.54000000e+02, 1.33683333e+03,\n",
       "         5.29442503e-03, 1.00000000e+00],\n",
       "        [2.28900000e+01, 3.06800000e+01, 4.54000000e+02, 1.34000000e+03,\n",
       "         5.30244234e-03, 1.00000000e+00],\n",
       "        [2.29450000e+01, 3.07000000e+01, 4.54000000e+02, 1.35075000e+03,\n",
       "         5.32378528e-03, 1.00000000e+00],\n",
       "        ...,\n",
       "        [2.27000000e+01, 3.13900000e+01, 4.47000000e+02, 1.39375000e+03,\n",
       "         5.36354037e-03, 1.00000000e+00],\n",
       "        [2.27000000e+01, 3.13900000e+01, 4.41400000e+02, 1.39260000e+03,\n",
       "         5.36354037e-03, 1.00000000e+00],\n",
       "        [2.27000000e+01, 3.13700000e+01, 4.38600000e+02, 1.39420000e+03,\n",
       "         5.36009356e-03, 1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2.06000000e+01, 2.22000000e+01, 0.00000000e+00, 4.54666667e+02,\n",
       "         3.32520581e-03, 0.00000000e+00],\n",
       "        [2.06000000e+01, 2.22540000e+01, 0.00000000e+00, 4.53200000e+02,\n",
       "         3.33333750e-03, 0.00000000e+00],\n",
       "        [2.05600000e+01, 2.22900000e+01, 0.00000000e+00, 4.47600000e+02,\n",
       "         3.33048912e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.05000000e+01, 2.22900000e+01, 0.00000000e+00, 4.40250000e+02,\n",
       "         3.31811853e-03, 0.00000000e+00],\n",
       "        [2.05000000e+01, 2.22900000e+01, 0.00000000e+00, 4.41500000e+02,\n",
       "         3.31811853e-03, 0.00000000e+00],\n",
       "        [2.05000000e+01, 2.22900000e+01, 0.00000000e+00, 4.39333333e+02,\n",
       "         3.31811853e-03, 0.00000000e+00]],\n",
       "\n",
       "       [[2.07000000e+01, 2.46000000e+01, 0.00000000e+00, 4.79500000e+02,\n",
       "         3.70974785e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.46000000e+01, 0.00000000e+00, 4.79750000e+02,\n",
       "         3.70974785e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.46000000e+01, 0.00000000e+00, 4.80750000e+02,\n",
       "         3.70974785e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.07000000e+01, 2.45000000e+01, 0.00000000e+00, 4.65666667e+02,\n",
       "         3.69457800e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.45000000e+01, 0.00000000e+00, 4.65750000e+02,\n",
       "         3.69457800e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.45000000e+01, 0.00000000e+00, 4.69750000e+02,\n",
       "         3.69457800e-03, 0.00000000e+00]],\n",
       "\n",
       "       [[2.07000000e+01, 2.68900000e+01, 0.00000000e+00, 1.27600000e+03,\n",
       "         4.05733882e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.67900000e+01, 0.00000000e+00, 1.27250000e+03,\n",
       "         4.04215212e-03, 0.00000000e+00],\n",
       "        [2.07000000e+01, 2.68233333e+01, 0.00000000e+00, 1.27200000e+03,\n",
       "         4.04721427e-03, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.07900000e+01, 2.61000000e+01, 0.00000000e+00, 1.41500000e+03,\n",
       "         3.95940883e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.62000000e+01, 0.00000000e+00, 1.43233333e+03,\n",
       "         3.97467591e-03, 0.00000000e+00],\n",
       "        [2.07900000e+01, 2.62000000e+01, 0.00000000e+00, 1.43250000e+03,\n",
       "         3.97467591e-03, 0.00000000e+00]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "scaler = TimeSeriesScalerMeanVariance(mu=0.,std=1.)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.33211094, -0.63281967,  0.75399493,  1.3418925 ,\n",
       "         -0.43956835,  0.55809982],\n",
       "        [ 1.32844377, -0.63281967,  0.69937727,  1.36833477,\n",
       "         -0.04862772,  0.55809982],\n",
       "        [ 0.83027736, -0.7634888 ,  0.76992509,  1.32073868,\n",
       "         -0.36202273,  0.55809982],\n",
       "        ...,\n",
       "        [ 0.33211094,  0.88294215, -1.79027784, -1.75714196,\n",
       "          0.92453087, -1.79179416],\n",
       "        [ 0.33211094,  0.88294215, -1.79027784, -1.70425742,\n",
       "          0.92453087, -1.79179416],\n",
       "        [ 0.33211094,  1.0745902 , -1.81758668, -1.63021905,\n",
       "          1.09701979, -1.79179416]],\n",
       "\n",
       "       [[ 1.59637296, -1.77331191,  1.82137546, -0.70519921,\n",
       "         -2.07270525,  0.        ],\n",
       "        [ 1.59637296, -1.80112351,  1.63008433, -0.87310778,\n",
       "         -2.15888369,  0.        ],\n",
       "        [ 1.59637296, -1.71768871,  1.63008433, -1.25593933,\n",
       "         -1.90034566,  0.        ],\n",
       "        ...,\n",
       "        [-2.18977237,  1.9635559 , -1.16754837,  1.7395496 ,\n",
       "          1.23165605,  0.        ],\n",
       "        [-1.90581147,  2.11272722, -1.57105309,  1.96790526,\n",
       "          2.27473118,  0.        ],\n",
       "        [-1.81115783,  2.07480231, -1.70555466,  1.12164605,\n",
       "          2.3665579 ,  0.        ]],\n",
       "\n",
       "       [[ 2.70509659, -1.9624941 ,  0.90033013,  1.90838781,\n",
       "         -0.02830414,  0.90453403],\n",
       "        [ 2.70509659, -1.9624941 ,  0.90033013,  1.68462479,\n",
       "         -0.02830414,  0.90453403],\n",
       "        [ 2.70509659, -1.50999321,  0.90033013,  1.68462479,\n",
       "          0.62645761,  0.90453403],\n",
       "        ...,\n",
       "        [-1.14853263,  0.01205526, -1.13357986, -1.83591346,\n",
       "         -1.16502602, -1.1055416 ],\n",
       "        [-0.46848042,  0.28629823, -1.13357986, -1.65690304,\n",
       "         -0.07248984, -1.1055416 ],\n",
       "        [-0.46848042,  0.14917675, -1.13357986, -1.98508881,\n",
       "         -0.26835954, -1.1055416 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.62568735, -1.04484625,  0.        ,  0.55075668,\n",
       "         -0.12641325,  0.        ],\n",
       "        [ 0.62568735, -1.04484625,  0.        ,  0.06119519,\n",
       "         -0.12641325,  0.        ],\n",
       "        [ 0.62568735, -1.04484625,  0.        ,  0.95872459,\n",
       "         -0.12641325,  0.        ],\n",
       "        ...,\n",
       "        [ 2.23016974, -1.04484625,  0.        , -1.28509891,\n",
       "          2.00194558,  0.        ],\n",
       "        [ 2.23016974, -1.04484625,  0.        , -1.3258957 ,\n",
       "          2.00194558,  0.        ],\n",
       "        [ 2.23016974, -1.04484625,  0.        , -0.67314705,\n",
       "          2.00194558,  0.        ]],\n",
       "\n",
       "       [[ 1.33366586, -1.8050303 ,  0.        , -2.45432588,\n",
       "         -1.75057589,  0.        ],\n",
       "        [ 1.33366586, -1.8050303 ,  0.        , -0.209384  ,\n",
       "         -1.75057589,  0.        ],\n",
       "        [ 1.33366586, -1.8050303 ,  0.        , -1.07282319,\n",
       "         -1.75057589,  0.        ],\n",
       "        ...,\n",
       "        [-0.83710258,  1.76650403,  0.        ,  2.03555788,\n",
       "          1.96874628,  0.        ],\n",
       "        [-2.13956364,  1.76650403,  0.        ,  0.76918041,\n",
       "          1.22776327,  0.        ],\n",
       "        [-0.83710258,  1.76650403,  0.        ,  0.91308694,\n",
       "          1.96874628,  0.        ]],\n",
       "\n",
       "       [[ 0.95980894, -1.94675   ,  0.        , -0.24674412,\n",
       "         -1.86618245,  0.        ],\n",
       "        [ 0.95980894, -1.75312381,  0.        , -0.63003596,\n",
       "         -1.65172065,  0.        ],\n",
       "        [ 0.95980894, -1.75312381,  0.        ,  0.32819364,\n",
       "         -1.65172065,  0.        ],\n",
       "        ...,\n",
       "        [-1.36700061,  1.57372621,  0.        , -0.43839004,\n",
       "          1.32274816,  0.        ],\n",
       "        [-1.36700061,  1.7673524 ,  0.        ,  0.71148548,\n",
       "          1.5359615 ,  0.        ],\n",
       "        [-2.53040538,  1.7673524 ,  0.        ,  1.09477732,\n",
       "          1.18115371,  0.        ]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(179, 80, 6)\n",
    "X_test = X_test.reshape(78, 80, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEPS:  80\n",
      "N. LABELS:  3\n",
      "N. FEATURES:  6\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_outputs, n_features = X_train.shape[1], len(np.unique(y_train)), X_train.shape[2] \n",
    "print(\"TIMESTEPS: \", n_timesteps)\n",
    "print(\"N. LABELS: \", n_outputs)\n",
    "print(\"N. FEATURES: \", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 80, 6)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n",
    "X_val_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.33211094, -0.63281967,  0.75399493,  1.3418925 ,\n",
       "         -0.43956835,  0.55809982],\n",
       "        [ 1.32844377, -0.63281967,  0.69937727,  1.36833477,\n",
       "         -0.04862772,  0.55809982],\n",
       "        [ 0.83027736, -0.7634888 ,  0.76992509,  1.32073868,\n",
       "         -0.36202273,  0.55809982],\n",
       "        ...,\n",
       "        [ 0.33211094,  0.88294215, -1.79027784, -1.75714196,\n",
       "          0.92453087, -1.79179416],\n",
       "        [ 0.33211094,  0.88294215, -1.79027784, -1.70425742,\n",
       "          0.92453087, -1.79179416],\n",
       "        [ 0.33211094,  1.0745902 , -1.81758668, -1.63021905,\n",
       "          1.09701979, -1.79179416]],\n",
       "\n",
       "       [[ 1.59637296, -1.77331191,  1.82137546, -0.70519921,\n",
       "         -2.07270525,  0.        ],\n",
       "        [ 1.59637296, -1.80112351,  1.63008433, -0.87310778,\n",
       "         -2.15888369,  0.        ],\n",
       "        [ 1.59637296, -1.71768871,  1.63008433, -1.25593933,\n",
       "         -1.90034566,  0.        ],\n",
       "        ...,\n",
       "        [-2.18977237,  1.9635559 , -1.16754837,  1.7395496 ,\n",
       "          1.23165605,  0.        ],\n",
       "        [-1.90581147,  2.11272722, -1.57105309,  1.96790526,\n",
       "          2.27473118,  0.        ],\n",
       "        [-1.81115783,  2.07480231, -1.70555466,  1.12164605,\n",
       "          2.3665579 ,  0.        ]],\n",
       "\n",
       "       [[ 2.70509659, -1.9624941 ,  0.90033013,  1.90838781,\n",
       "         -0.02830414,  0.90453403],\n",
       "        [ 2.70509659, -1.9624941 ,  0.90033013,  1.68462479,\n",
       "         -0.02830414,  0.90453403],\n",
       "        [ 2.70509659, -1.50999321,  0.90033013,  1.68462479,\n",
       "          0.62645761,  0.90453403],\n",
       "        ...,\n",
       "        [-1.14853263,  0.01205526, -1.13357986, -1.83591346,\n",
       "         -1.16502602, -1.1055416 ],\n",
       "        [-0.46848042,  0.28629823, -1.13357986, -1.65690304,\n",
       "         -0.07248984, -1.1055416 ],\n",
       "        [-0.46848042,  0.14917675, -1.13357986, -1.98508881,\n",
       "         -0.26835954, -1.1055416 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.62568735, -1.04484625,  0.        ,  0.55075668,\n",
       "         -0.12641325,  0.        ],\n",
       "        [ 0.62568735, -1.04484625,  0.        ,  0.06119519,\n",
       "         -0.12641325,  0.        ],\n",
       "        [ 0.62568735, -1.04484625,  0.        ,  0.95872459,\n",
       "         -0.12641325,  0.        ],\n",
       "        ...,\n",
       "        [ 2.23016974, -1.04484625,  0.        , -1.28509891,\n",
       "          2.00194558,  0.        ],\n",
       "        [ 2.23016974, -1.04484625,  0.        , -1.3258957 ,\n",
       "          2.00194558,  0.        ],\n",
       "        [ 2.23016974, -1.04484625,  0.        , -0.67314705,\n",
       "          2.00194558,  0.        ]],\n",
       "\n",
       "       [[ 1.33366586, -1.8050303 ,  0.        , -2.45432588,\n",
       "         -1.75057589,  0.        ],\n",
       "        [ 1.33366586, -1.8050303 ,  0.        , -0.209384  ,\n",
       "         -1.75057589,  0.        ],\n",
       "        [ 1.33366586, -1.8050303 ,  0.        , -1.07282319,\n",
       "         -1.75057589,  0.        ],\n",
       "        ...,\n",
       "        [-0.83710258,  1.76650403,  0.        ,  2.03555788,\n",
       "          1.96874628,  0.        ],\n",
       "        [-2.13956364,  1.76650403,  0.        ,  0.76918041,\n",
       "          1.22776327,  0.        ],\n",
       "        [-0.83710258,  1.76650403,  0.        ,  0.91308694,\n",
       "          1.96874628,  0.        ]],\n",
       "\n",
       "       [[ 0.95980894, -1.94675   ,  0.        , -0.24674412,\n",
       "         -1.86618245,  0.        ],\n",
       "        [ 0.95980894, -1.75312381,  0.        , -0.63003596,\n",
       "         -1.65172065,  0.        ],\n",
       "        [ 0.95980894, -1.75312381,  0.        ,  0.32819364,\n",
       "         -1.65172065,  0.        ],\n",
       "        ...,\n",
       "        [-1.36700061,  1.57372621,  0.        , -0.43839004,\n",
       "          1.32274816,  0.        ],\n",
       "        [-1.36700061,  1.7673524 ,  0.        ,  0.71148548,\n",
       "          1.5359615 ,  0.        ],\n",
       "        [-2.53040538,  1.7673524 ,  0.        ,  1.09477732,\n",
       "          1.18115371,  0.        ]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, Conv2D, MaxPool2D, Flatten, Dropout, LeakyReLU, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm2(n_timesteps, n_outputs, n_features):\n",
    "    model = Sequential()\n",
    "    #LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2])\n",
    "    model.add(LSTM(6, input_shape=(n_timesteps, n_features), return_sequences=True, \n",
    "                        kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU()) #activationfunction\n",
    "    model.add(Dropout(0.8)) #regolarizzazione to avoid overfitting\n",
    "                            #The term “dropout” refers to dropping out units (hidden and visible) in a neural network.\n",
    "    \n",
    "    #1\n",
    "    for _ in range(2):\n",
    "        model.add(LSTM(143, kernel_initializer='TruncatedNormal', return_sequences=True))\n",
    "        model.add(BatchNormalization())#only the first layer of the model requires the input dimension to be explicitly stated; \n",
    "                                    #the following layers are able to infer from the previous linear stacked lay\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.05))   \n",
    "\n",
    "    #2\n",
    "    model.add(LSTM(143, kernel_initializer='TruncatedNormal', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    #3\n",
    "#    for _ in range(2):\n",
    "#        model.add(Dense(256, kernel_initializer='TruncatedNormal', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))) #hidden layer con 256 nodi\n",
    "#        model.add(BatchNormalization())\n",
    "#        model.add(LeakyReLU())\n",
    "#        model.add(Dropout(0.5))\n",
    "#    #4\n",
    "#    for _ in range(1):\n",
    "#        model.add(Dense(64, kernel_initializer='TruncatedNormal'))\n",
    "#        model.add(BatchNormalization())\n",
    "#        model.add(LeakyReLU())\n",
    "#        model.add(Dropout(0.5))\n",
    "#\n",
    "#    #5\n",
    "#    model.add(Dense(32, kernel_initializer='TruncatedNormal'))\n",
    "#    model.add(BatchNormalization())\n",
    "#    model.add(LeakyReLU())\n",
    "#    model.add(Dropout(0.7))\n",
    "        \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    adam = optimizers.Adam(lr=1e-5)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = build_lstm2(n_timesteps, n_outputs, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 80, 6)             312       \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 80, 6)             24        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 80, 6)             0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 80, 6)             0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 80, 143)           85800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 80, 143)           572       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 80, 143)           0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 80, 143)           0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 80, 143)           164164    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 80, 143)           572       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 80, 143)           0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 80, 143)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 143)               164164    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 143)               572       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 143)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 143)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 432       \n",
      "=================================================================\n",
      "Total params: 416,612\n",
      "Trainable params: 415,742\n",
      "Non-trainable params: 870\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='accuracy', factor=0.2, patience=10, verbose=0, mode='auto',min_delta=0, cooldown=0, min_lr=0.1,)\n",
    "mc = ModelCheckpoint('best_model_lstm2.h5', monitor='loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 50\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143 samples, validate on 36 samples\n",
      "Epoch 1/50\n",
      "143/143 [==============================] - 20s 137ms/step - loss: 0.2912 - accuracy: 0.8741 - val_loss: 1.7223 - val_accuracy: 0.5833\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 14s 95ms/step - loss: 0.4155 - accuracy: 0.8392 - val_loss: 1.0889 - val_accuracy: 0.6111\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 14s 96ms/step - loss: 0.3387 - accuracy: 0.8741 - val_loss: 1.8246 - val_accuracy: 0.5556\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 14s 96ms/step - loss: 0.3776 - accuracy: 0.8252 - val_loss: 1.9890 - val_accuracy: 0.6111\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 12s 84ms/step - loss: 0.3570 - accuracy: 0.8322 - val_loss: 1.6312 - val_accuracy: 0.5833\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 12s 84ms/step - loss: 0.3524 - accuracy: 0.8601 - val_loss: 1.2194 - val_accuracy: 0.6111\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 13s 93ms/step - loss: 0.3004 - accuracy: 0.9091 - val_loss: 1.2252 - val_accuracy: 0.6667\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 18s 123ms/step - loss: 0.3195 - accuracy: 0.8811 - val_loss: 0.9330 - val_accuracy: 0.7500\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.2868 - accuracy: 0.9021 - val_loss: 0.6837 - val_accuracy: 0.8056\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 15s 105ms/step - loss: 0.2847 - accuracy: 0.8951 - val_loss: 1.1808 - val_accuracy: 0.6667\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 13s 94ms/step - loss: 0.3485 - accuracy: 0.8671 - val_loss: 1.1874 - val_accuracy: 0.6667\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 15s 102ms/step - loss: 0.3102 - accuracy: 0.8881 - val_loss: 1.0510 - val_accuracy: 0.7222\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 15s 105ms/step - loss: 0.2015 - accuracy: 0.9301 - val_loss: 1.0035 - val_accuracy: 0.7500\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 14s 98ms/step - loss: 0.2418 - accuracy: 0.8671 - val_loss: 0.9435 - val_accuracy: 0.7778\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 18s 125ms/step - loss: 0.2923 - accuracy: 0.9231 - val_loss: 0.9763 - val_accuracy: 0.7222\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 19s 130ms/step - loss: 0.2288 - accuracy: 0.9371 - val_loss: 1.1269 - val_accuracy: 0.6944\n",
      "Epoch 17/50\n",
      "143/143 [==============================] - 20s 138ms/step - loss: 0.1722 - accuracy: 0.9231 - val_loss: 0.9887 - val_accuracy: 0.7500\n",
      "Epoch 18/50\n",
      "143/143 [==============================] - 19s 133ms/step - loss: 0.2186 - accuracy: 0.8881 - val_loss: 0.9381 - val_accuracy: 0.7222\n",
      "Epoch 19/50\n",
      "143/143 [==============================] - 19s 136ms/step - loss: 0.2287 - accuracy: 0.9301 - val_loss: 0.8975 - val_accuracy: 0.7500\n",
      "Epoch 20/50\n",
      "143/143 [==============================] - 20s 140ms/step - loss: 0.1615 - accuracy: 0.9441 - val_loss: 0.9744 - val_accuracy: 0.7778\n",
      "Epoch 21/50\n",
      "143/143 [==============================] - 14s 97ms/step - loss: 0.2079 - accuracy: 0.9161 - val_loss: 0.8742 - val_accuracy: 0.8056\n",
      "Epoch 22/50\n",
      "143/143 [==============================] - 16s 113ms/step - loss: 0.1932 - accuracy: 0.9371 - val_loss: 1.0162 - val_accuracy: 0.7778\n",
      "Epoch 23/50\n",
      "143/143 [==============================] - 16s 115ms/step - loss: 0.3202 - accuracy: 0.8811 - val_loss: 1.0206 - val_accuracy: 0.7222\n",
      "Epoch 24/50\n",
      "143/143 [==============================] - 16s 111ms/step - loss: 0.3511 - accuracy: 0.8601 - val_loss: 0.7759 - val_accuracy: 0.8056\n",
      "Epoch 25/50\n",
      "143/143 [==============================] - 17s 120ms/step - loss: 0.4451 - accuracy: 0.8601 - val_loss: 0.8005 - val_accuracy: 0.7778\n",
      "Epoch 26/50\n",
      "143/143 [==============================] - 14s 101ms/step - loss: 0.3049 - accuracy: 0.8811 - val_loss: 0.7975 - val_accuracy: 0.8333\n",
      "Epoch 27/50\n",
      "143/143 [==============================] - 14s 99ms/step - loss: 0.1869 - accuracy: 0.9231 - val_loss: 0.8639 - val_accuracy: 0.7500\n",
      "Epoch 28/50\n",
      "143/143 [==============================] - 17s 121ms/step - loss: 0.2613 - accuracy: 0.9231 - val_loss: 0.8114 - val_accuracy: 0.7500\n",
      "Epoch 29/50\n",
      "143/143 [==============================] - 15s 104ms/step - loss: 0.2794 - accuracy: 0.9371 - val_loss: 0.7543 - val_accuracy: 0.7778\n",
      "Epoch 30/50\n",
      "143/143 [==============================] - 15s 106ms/step - loss: 0.1738 - accuracy: 0.9301 - val_loss: 0.6568 - val_accuracy: 0.8333\n",
      "Epoch 31/50\n",
      "143/143 [==============================] - 22s 155ms/step - loss: 0.1937 - accuracy: 0.9231 - val_loss: 0.7462 - val_accuracy: 0.7778\n",
      "Epoch 32/50\n",
      "143/143 [==============================] - 16s 114ms/step - loss: 0.3443 - accuracy: 0.9161 - val_loss: 0.7291 - val_accuracy: 0.8333\n",
      "Epoch 33/50\n",
      "143/143 [==============================] - 13s 91ms/step - loss: 0.1906 - accuracy: 0.9231 - val_loss: 0.5618 - val_accuracy: 0.8333\n",
      "Epoch 34/50\n",
      "143/143 [==============================] - 17s 115ms/step - loss: 0.2159 - accuracy: 0.9371 - val_loss: 0.6394 - val_accuracy: 0.8333\n",
      "Epoch 35/50\n",
      "143/143 [==============================] - 18s 125ms/step - loss: 0.1909 - accuracy: 0.9301 - val_loss: 0.5864 - val_accuracy: 0.8611\n",
      "Epoch 36/50\n",
      "143/143 [==============================] - 13s 89ms/step - loss: 0.3093 - accuracy: 0.9371 - val_loss: 0.5062 - val_accuracy: 0.8889\n",
      "Epoch 37/50\n",
      "143/143 [==============================] - 15s 102ms/step - loss: 0.2230 - accuracy: 0.9441 - val_loss: 0.5869 - val_accuracy: 0.8333\n",
      "Epoch 38/50\n",
      "143/143 [==============================] - 16s 114ms/step - loss: 0.1815 - accuracy: 0.9510 - val_loss: 0.6320 - val_accuracy: 0.8333\n",
      "Epoch 39/50\n",
      "143/143 [==============================] - 15s 105ms/step - loss: 0.1976 - accuracy: 0.9231 - val_loss: 0.6479 - val_accuracy: 0.8333\n",
      "Epoch 40/50\n",
      "143/143 [==============================] - 17s 117ms/step - loss: 0.2247 - accuracy: 0.9301 - val_loss: 0.6741 - val_accuracy: 0.8333\n",
      "Epoch 41/50\n",
      "143/143 [==============================] - 15s 102ms/step - loss: 0.2890 - accuracy: 0.9091 - val_loss: 0.5457 - val_accuracy: 0.8333\n",
      "Epoch 42/50\n",
      "143/143 [==============================] - 16s 110ms/step - loss: 0.1631 - accuracy: 0.9371 - val_loss: 0.6097 - val_accuracy: 0.8333\n",
      "Epoch 43/50\n",
      "143/143 [==============================] - 17s 117ms/step - loss: 0.1418 - accuracy: 0.9441 - val_loss: 0.8221 - val_accuracy: 0.8056\n",
      "Epoch 44/50\n",
      "143/143 [==============================] - 15s 101ms/step - loss: 0.2151 - accuracy: 0.9301 - val_loss: 0.6742 - val_accuracy: 0.8333\n",
      "Epoch 45/50\n",
      "143/143 [==============================] - 12s 84ms/step - loss: 0.3054 - accuracy: 0.8951 - val_loss: 0.6840 - val_accuracy: 0.8333\n",
      "Epoch 46/50\n",
      "143/143 [==============================] - 11s 79ms/step - loss: 0.3511 - accuracy: 0.8531 - val_loss: 0.5611 - val_accuracy: 0.7778\n",
      "Epoch 47/50\n",
      "143/143 [==============================] - 12s 87ms/step - loss: 0.2673 - accuracy: 0.9231 - val_loss: 0.6426 - val_accuracy: 0.8333\n",
      "Epoch 48/50\n",
      "143/143 [==============================] - 12s 86ms/step - loss: 0.1764 - accuracy: 0.9720 - val_loss: 0.8031 - val_accuracy: 0.8611\n",
      "Epoch 49/50\n",
      "143/143 [==============================] - 12s 81ms/step - loss: 0.2466 - accuracy: 0.9231 - val_loss: 0.7005 - val_accuracy: 0.8611\n",
      "Epoch 50/50\n",
      "143/143 [==============================] - 11s 79ms/step - loss: 0.3057 - accuracy: 0.9021 - val_loss: 0.6716 - val_accuracy: 0.8056\n"
     ]
    }
   ],
   "source": [
    "history_lstm2 = lstm2.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=15, callbacks=callbacks,\n",
    "                          validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6923076923076923\n",
      "F1-score [0.63829787 0.84782609 0.        ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.94      0.64        16\n",
      "           1       0.87      0.83      0.85        47\n",
      "           2       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.69        78\n",
      "   macro avg       0.45      0.59      0.50        78\n",
      "weighted avg       0.62      0.69      0.64        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(lstm2.predict(X_test), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 80, 6)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 80, 6, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn2 = X_train_cnn.reshape(X_train_cnn.shape[0], X_train_cnn.shape[1], X_train_cnn.shape[2], 1)\n",
    "X_val_cnn2 = X_val_cnn.reshape(X_val_cnn.shape[0], X_val_cnn.shape[1], X_val_cnn.shape[2], 1)\n",
    "X_test_cnn2 = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "X_train_cnn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn2(n_timesteps, n_features, n_outputs):\n",
    "    input_shape = (n_timesteps, n_features, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ks1_first = 3\n",
    "    ks1_second = 3\n",
    "    \n",
    "    ks2_first = 4\n",
    "    ks2_second = 4\n",
    "    \n",
    "    model.add(Conv2D(filters=(3), \n",
    "                     kernel_size=(ks1_first, ks1_second),\n",
    "                     input_shape=input_shape, \n",
    "                     padding='same',\n",
    "                     kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.02))\n",
    "    \n",
    "    for _ in range(2):\n",
    "        model.add(Conv2D(filters=(4), \n",
    "                     kernel_size= (ks2_first, ks2_second), \n",
    "                         padding='same',\n",
    "                     kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.2))  \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(4):\n",
    "        model.add(Dense(64 , kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.4))\n",
    "    \n",
    "    for _ in range(3):\n",
    "        model.add(Dense(128 , kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(1024 , kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.7))\n",
    "        \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = build_cnn2(n_timesteps, n_features, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143 samples, validate on 36 samples\n",
      "Epoch 1/50\n",
      "143/143 [==============================] - 12s 83ms/step - loss: 1.2797 - accuracy: 0.4196 - val_loss: 1.0772 - val_accuracy: 0.5833\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0736 - accuracy: 0.5245 - val_loss: 1.0649 - val_accuracy: 0.5833\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.1755 - accuracy: 0.4895 - val_loss: 1.0569 - val_accuracy: 0.5833\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0178 - accuracy: 0.5175 - val_loss: 1.0501 - val_accuracy: 0.5833\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.1128 - accuracy: 0.5524 - val_loss: 1.0433 - val_accuracy: 0.5833\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0164 - accuracy: 0.6084 - val_loss: 1.0405 - val_accuracy: 0.5833\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0196 - accuracy: 0.5455 - val_loss: 1.0406 - val_accuracy: 0.5833\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9825 - accuracy: 0.5245 - val_loss: 1.0422 - val_accuracy: 0.5833\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9853 - accuracy: 0.5804 - val_loss: 1.0430 - val_accuracy: 0.5833\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9456 - accuracy: 0.6014 - val_loss: 1.0467 - val_accuracy: 0.5833\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8903 - accuracy: 0.5734 - val_loss: 1.0544 - val_accuracy: 0.5833\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9515 - accuracy: 0.6224 - val_loss: 1.0583 - val_accuracy: 0.5833\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8524 - accuracy: 0.5874 - val_loss: 1.0576 - val_accuracy: 0.5833\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8697 - accuracy: 0.5804 - val_loss: 1.0569 - val_accuracy: 0.5833\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.8690 - accuracy: 0.6014 - val_loss: 1.0535 - val_accuracy: 0.5833\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.7898 - accuracy: 0.6434 - val_loss: 1.0551 - val_accuracy: 0.5833\n",
      "Epoch 17/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.8327 - accuracy: 0.6154 - val_loss: 1.0554 - val_accuracy: 0.5833\n",
      "Epoch 18/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.8087 - accuracy: 0.6154 - val_loss: 1.0543 - val_accuracy: 0.5833\n",
      "Epoch 19/50\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.7984 - accuracy: 0.6224 - val_loss: 1.0564 - val_accuracy: 0.5833\n",
      "Epoch 20/50\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.9535 - accuracy: 0.5594 - val_loss: 1.0578 - val_accuracy: 0.5833\n",
      "Epoch 21/50\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.7416 - accuracy: 0.6294 - val_loss: 1.0602 - val_accuracy: 0.5833\n",
      "Epoch 22/50\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.7502 - accuracy: 0.6573 - val_loss: 1.0672 - val_accuracy: 0.5833\n",
      "Epoch 23/50\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.8068 - accuracy: 0.6783 - val_loss: 1.0670 - val_accuracy: 0.5833\n",
      "Epoch 24/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.7554 - accuracy: 0.6434 - val_loss: 1.0685 - val_accuracy: 0.5833\n",
      "Epoch 25/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.6644 - accuracy: 0.6573 - val_loss: 1.0735 - val_accuracy: 0.6389\n",
      "Epoch 26/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.7830 - accuracy: 0.7273 - val_loss: 1.0819 - val_accuracy: 0.3333\n",
      "Epoch 27/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.6983 - accuracy: 0.7273 - val_loss: 1.0811 - val_accuracy: 0.3056\n",
      "Epoch 28/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.6497 - accuracy: 0.7063 - val_loss: 1.0811 - val_accuracy: 0.3056\n",
      "Epoch 29/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.6242 - accuracy: 0.6993 - val_loss: 1.0796 - val_accuracy: 0.2778\n",
      "Epoch 30/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5722 - accuracy: 0.7203 - val_loss: 1.0854 - val_accuracy: 0.2500\n",
      "Epoch 31/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5127 - accuracy: 0.7972 - val_loss: 1.0893 - val_accuracy: 0.2500\n",
      "Epoch 32/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5488 - accuracy: 0.7413 - val_loss: 1.1148 - val_accuracy: 0.2222\n",
      "Epoch 33/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.5110 - accuracy: 0.7063 - val_loss: 1.2122 - val_accuracy: 0.2222\n",
      "Epoch 34/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.6522 - accuracy: 0.7203 - val_loss: 1.1092 - val_accuracy: 0.3056\n",
      "Epoch 35/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.5613 - accuracy: 0.7063 - val_loss: 1.0407 - val_accuracy: 0.3611\n",
      "Epoch 36/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.5169 - accuracy: 0.7063 - val_loss: 0.9786 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.4382 - accuracy: 0.7762 - val_loss: 0.9416 - val_accuracy: 0.6389\n",
      "Epoch 38/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5417 - accuracy: 0.7203 - val_loss: 0.9065 - val_accuracy: 0.6389\n",
      "Epoch 39/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.4041 - accuracy: 0.7692 - val_loss: 0.8964 - val_accuracy: 0.6389\n",
      "Epoch 40/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.4407 - accuracy: 0.7133 - val_loss: 0.8933 - val_accuracy: 0.6667\n",
      "Epoch 41/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.4249 - accuracy: 0.7343 - val_loss: 0.8953 - val_accuracy: 0.7222\n",
      "Epoch 42/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.3598 - accuracy: 0.7832 - val_loss: 0.9307 - val_accuracy: 0.6944\n",
      "Epoch 43/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.4250 - accuracy: 0.7902 - val_loss: 0.9276 - val_accuracy: 0.6944\n",
      "Epoch 44/50\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.3847 - accuracy: 0.7832 - val_loss: 0.9422 - val_accuracy: 0.7500\n",
      "Epoch 45/50\n",
      "143/143 [==============================] - 1s 8ms/step - loss: 0.3959 - accuracy: 0.7622 - val_loss: 0.8763 - val_accuracy: 0.7222\n",
      "Epoch 46/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.5057 - accuracy: 0.7483 - val_loss: 0.8293 - val_accuracy: 0.7222\n",
      "Epoch 47/50\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.5054 - accuracy: 0.7622 - val_loss: 0.7806 - val_accuracy: 0.7500\n",
      "Epoch 48/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.3830 - accuracy: 0.8182 - val_loss: 0.6745 - val_accuracy: 0.7778\n",
      "Epoch 49/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.3382 - accuracy: 0.7902 - val_loss: 0.6571 - val_accuracy: 0.7778\n",
      "Epoch 50/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.4130 - accuracy: 0.7902 - val_loss: 0.6336 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "history_cnn2 = cnn2.fit(X_train_cnn2, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn2, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.782051282051282\n",
      "F1-score [0.66666667 0.96774194 0.        ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67        16\n",
      "           1       0.98      0.96      0.97        47\n",
      "           2       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.78        78\n",
      "   macro avg       0.49      0.65      0.54        78\n",
      "weighted avg       0.69      0.78      0.72        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn2.predict(X_test_cnn2), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn3(n_timesteps, n_outputs, n_features):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=16, kernel_size=8, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3 = build_cnn3(n_timesteps, n_outputs, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143 samples, validate on 36 samples\n",
      "Epoch 1/50\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 0.9788 - accuracy: 0.6224 - val_loss: 1.0445 - val_accuracy: 0.7222\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.8556 - accuracy: 0.7552 - val_loss: 1.0077 - val_accuracy: 0.7500\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.7484 - accuracy: 0.7832 - val_loss: 0.9723 - val_accuracy: 0.7778\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.6800 - accuracy: 0.7692 - val_loss: 0.9492 - val_accuracy: 0.8056\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.6192 - accuracy: 0.8392 - val_loss: 0.9241 - val_accuracy: 0.7778\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5864 - accuracy: 0.8462 - val_loss: 0.9097 - val_accuracy: 0.7500\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.5201 - accuracy: 0.8392 - val_loss: 0.8974 - val_accuracy: 0.7778\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.4888 - accuracy: 0.8951 - val_loss: 0.8979 - val_accuracy: 0.8056\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.4410 - accuracy: 0.8741 - val_loss: 0.8877 - val_accuracy: 0.8333\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.4055 - accuracy: 0.9091 - val_loss: 0.8739 - val_accuracy: 0.8333\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.4101 - accuracy: 0.8462 - val_loss: 0.8767 - val_accuracy: 0.8333\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.3510 - accuracy: 0.9231 - val_loss: 0.8785 - val_accuracy: 0.8056\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.3452 - accuracy: 0.9091 - val_loss: 0.8723 - val_accuracy: 0.6944\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.2978 - accuracy: 0.9161 - val_loss: 0.8492 - val_accuracy: 0.6944\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.2831 - accuracy: 0.9371 - val_loss: 0.8400 - val_accuracy: 0.7778\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.2449 - accuracy: 0.9580 - val_loss: 0.8391 - val_accuracy: 0.6944\n",
      "Epoch 17/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.2455 - accuracy: 0.9231 - val_loss: 0.8279 - val_accuracy: 0.6944\n",
      "Epoch 18/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.2173 - accuracy: 0.9441 - val_loss: 0.8048 - val_accuracy: 0.6667\n",
      "Epoch 19/50\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.2402 - accuracy: 0.9301 - val_loss: 0.7879 - val_accuracy: 0.6389\n",
      "Epoch 20/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.1973 - accuracy: 0.9441 - val_loss: 0.7388 - val_accuracy: 0.7778\n",
      "Epoch 21/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.1960 - accuracy: 0.9301 - val_loss: 0.7342 - val_accuracy: 0.7778\n",
      "Epoch 22/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.1710 - accuracy: 0.9510 - val_loss: 0.7148 - val_accuracy: 0.8056\n",
      "Epoch 23/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.2029 - accuracy: 0.9231 - val_loss: 0.6986 - val_accuracy: 0.8333\n",
      "Epoch 24/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.1655 - accuracy: 0.9510 - val_loss: 0.7160 - val_accuracy: 0.7500\n",
      "Epoch 25/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.1842 - accuracy: 0.9580 - val_loss: 0.7559 - val_accuracy: 0.7222\n",
      "Epoch 26/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.1830 - accuracy: 0.9441 - val_loss: 0.7124 - val_accuracy: 0.7500\n",
      "Epoch 27/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.1585 - accuracy: 0.9650 - val_loss: 0.7293 - val_accuracy: 0.6111\n",
      "Epoch 28/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.9790 - val_loss: 0.6771 - val_accuracy: 0.8333\n",
      "Epoch 29/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.1433 - accuracy: 0.9580 - val_loss: 0.6375 - val_accuracy: 0.8611\n",
      "Epoch 30/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.1619 - accuracy: 0.9650 - val_loss: 0.6344 - val_accuracy: 0.7778\n",
      "Epoch 31/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.0973 - accuracy: 0.9860 - val_loss: 0.6172 - val_accuracy: 0.8611\n",
      "Epoch 32/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.0996 - accuracy: 0.9720 - val_loss: 0.6112 - val_accuracy: 0.8333\n",
      "Epoch 33/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.1179 - accuracy: 0.9580 - val_loss: 0.5912 - val_accuracy: 0.8333\n",
      "Epoch 34/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0901 - accuracy: 0.9860 - val_loss: 0.5616 - val_accuracy: 0.8333\n",
      "Epoch 35/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0870 - accuracy: 0.9860 - val_loss: 0.5490 - val_accuracy: 0.8611\n",
      "Epoch 36/50\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9930 - val_loss: 0.5377 - val_accuracy: 0.8611\n",
      "Epoch 37/50\n",
      "143/143 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.96 - 1s 4ms/step - loss: 0.0943 - accuracy: 0.9720 - val_loss: 0.5129 - val_accuracy: 0.8611\n",
      "Epoch 38/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0611 - accuracy: 1.0000 - val_loss: 0.4990 - val_accuracy: 0.8333\n",
      "Epoch 39/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.0913 - accuracy: 0.9650 - val_loss: 0.4861 - val_accuracy: 0.8611\n",
      "Epoch 40/50\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.0686 - accuracy: 0.9930 - val_loss: 0.4759 - val_accuracy: 0.8333\n",
      "Epoch 41/50\n",
      "143/143 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9790 - val_loss: 0.4802 - val_accuracy: 0.8611\n",
      "Epoch 42/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0704 - accuracy: 0.9790 - val_loss: 0.4979 - val_accuracy: 0.8333\n",
      "Epoch 43/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0931 - accuracy: 0.9720 - val_loss: 0.4908 - val_accuracy: 0.8333\n",
      "Epoch 44/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0773 - accuracy: 0.9790 - val_loss: 0.4702 - val_accuracy: 0.8611\n",
      "Epoch 45/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0713 - accuracy: 0.9790 - val_loss: 0.5060 - val_accuracy: 0.8333\n",
      "Epoch 46/50\n",
      "143/143 [==============================] - 1s 4ms/step - loss: 0.0588 - accuracy: 0.9790 - val_loss: 0.4620 - val_accuracy: 0.7778\n",
      "Epoch 47/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0707 - accuracy: 0.9860 - val_loss: 0.4499 - val_accuracy: 0.7500\n",
      "Epoch 48/50\n",
      "143/143 [==============================] - 1s 5ms/step - loss: 0.0482 - accuracy: 0.9930 - val_loss: 0.4532 - val_accuracy: 0.7500\n",
      "Epoch 49/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.0548 - accuracy: 0.9930 - val_loss: 0.4696 - val_accuracy: 0.8056\n",
      "Epoch 50/50\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.5478 - val_accuracy: 0.8056\n"
     ]
    }
   ],
   "source": [
    "history_cnn3 = cnn3.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9102564102564102\n",
      "F1-score [0.89655172 0.95744681 0.78787879]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.81      0.90        16\n",
      "           1       0.96      0.96      0.96        47\n",
      "           2       0.72      0.87      0.79        15\n",
      "\n",
      "    accuracy                           0.91        78\n",
      "   macro avg       0.89      0.88      0.88        78\n",
      "weighted avg       0.92      0.91      0.91        78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn3.predict(X_test), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
